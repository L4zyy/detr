Timer unit: 1e-06 s

Total time: 0.037375 s
File: /home/zhiyilai/.cache/torch/hub/facebookresearch_detr_master/models/transformer.py
Function: forward at line 95

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    95                                               def forward(self, tgt, memory,
    96                                                           tgt_mask: Optional[Tensor] = None,
    97                                                           memory_mask: Optional[Tensor] = None,
    98                                                           tgt_key_padding_mask: Optional[Tensor] = None,
    99                                                           memory_key_padding_mask: Optional[Tensor] = None,
   100                                                           pos: Optional[Tensor] = None,
   101                                                           query_pos: Optional[Tensor] = None):
   102         1          3.0      3.0      0.0          output = tgt
   103                                           
   104         1          2.0      2.0      0.0          intermediate = []
   105                                           
   106         7         37.0      5.3      0.1          for layer in self.layers:
   107         6          8.0      1.3      0.0              output = layer(output, memory, tgt_mask=tgt_mask,
   108         6          6.0      1.0      0.0                             memory_mask=memory_mask,
   109         6          8.0      1.3      0.0                             tgt_key_padding_mask=tgt_key_padding_mask,
   110         6          7.0      1.2      0.0                             memory_key_padding_mask=memory_key_padding_mask,
   111         6      36228.0   6038.0     96.9                             pos=pos, query_pos=query_pos)
   112         6         22.0      3.7      0.1              if self.return_intermediate:
   113         6        704.0    117.3      1.9                  intermediate.append(self.norm(output))
   114                                           
   115         1         21.0     21.0      0.1          if self.norm is not None:
   116         1        103.0    103.0      0.3              output = self.norm(output)
   117         1         20.0     20.0      0.1              if self.return_intermediate:
   118         1         11.0     11.0      0.0                  intermediate.pop()
   119         1          1.0      1.0      0.0                  intermediate.append(output)
   120                                           
   121         1          1.0      1.0      0.0          if self.return_intermediate:
   122         1        193.0    193.0      0.5              return torch.stack(intermediate)
   123                                           
   124                                                   return output.unsqueeze(0)